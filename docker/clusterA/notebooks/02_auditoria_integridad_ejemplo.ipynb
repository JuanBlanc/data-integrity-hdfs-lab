{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1169d7",
   "metadata": {},
   "source": [
    "# Auditoría de Integridad y Métricas — DataSecure Lab\n",
    "\n",
    "Este notebook presenta las métricas reales generadas durante la ejecución del pipeline de integridad de datos en HDFS.\n",
    "\n",
    "**Fecha de ejecución:** 2026-02-13  \n",
    "**Clúster:** 3 DataNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83a3d2",
   "metadata": {},
   "source": [
    "## 1) Configuración y carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9aa5497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "VERIFICACIÓN DE DIRECTORIOS\n",
      "==================================================\n",
      "AUDIT_DIR exists:     True\n",
      "INVENTORY_DIR exists: True\n",
      "METRICS_DIR exists:   True\n",
      "\n",
      "Auditorías disponibles:\n",
      "  - 2026-02-13/fsck_data.txt\n",
      "  - 2026-02-13/fsck_post_incident.txt\n",
      "  - 2026-02-13/fsck_pre_incident.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Rutas a los directorios de auditoría\n",
    "AUDIT_DIR = Path('/media/notebooks/audit/fsck')\n",
    "INVENTORY_DIR = Path('/media/notebooks/audit/inventory')\n",
    "METRICS_DIR = Path('/media/notebooks/audit/metrics')\n",
    "\n",
    "print('=' * 50)\n",
    "print('VERIFICACIÓN DE DIRECTORIOS')\n",
    "print('=' * 50)\n",
    "print(f'AUDIT_DIR exists:     {AUDIT_DIR.exists()}')\n",
    "print(f'INVENTORY_DIR exists: {INVENTORY_DIR.exists()}')\n",
    "print(f'METRICS_DIR exists:   {METRICS_DIR.exists()}')\n",
    "\n",
    "# Listar auditorías disponibles\n",
    "if AUDIT_DIR.exists():\n",
    "    print(f'\\nAuditorías disponibles:')\n",
    "    for f in sorted(AUDIT_DIR.rglob('*.txt')):\n",
    "        print(f'  - {f.relative_to(AUDIT_DIR)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b86c0",
   "metadata": {},
   "source": [
    "## 2) Resumen de Auditoría FSCK\n",
    "\n",
    "Análisis del estado de integridad del sistema de ficheros HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a88231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RESUMEN DE AUDITORÍA FSCK - /data\n",
      "==================================================\n",
      "Total size (bytes)        : 816636433\n",
      "Total files               : 2\n",
      "Total blocks              : 13\n",
      "DataNodes                 : 3\n",
      "Corrupt blocks            : 0\n",
      "Missing blocks            : 0\n",
      "Under-replicated          : 0\n",
      "Replication factor        : 3\n",
      "Status                    : HEALTHY\n"
     ]
    }
   ],
   "source": [
    "def parse_fsck(text):\n",
    "    \"\"\"Extrae métricas de una salida de hdfs fsck.\"\"\"\n",
    "    def extract(pattern, default='0'):\n",
    "        match = re.search(pattern, text)\n",
    "        return match.group(1) if match else default\n",
    "    \n",
    "    return {\n",
    "        'Total size (bytes)': extract(r'Total size:\\s+(\\d+)'),\n",
    "        'Total files': extract(r'Total files:\\s+(\\d+)'),\n",
    "        'Total blocks': extract(r'Total blocks[^:]*:\\s+(\\d+)'),\n",
    "        'DataNodes': extract(r'Number of data-nodes:\\s+(\\d+)'),\n",
    "        'Corrupt blocks': extract(r'Corrupt blocks:\\s+(\\d+)'),\n",
    "        'Missing blocks': extract(r'Missing blocks:\\s+(\\d+)'),\n",
    "        'Under-replicated': extract(r'Under-replicated blocks:\\s+(\\d+)'),\n",
    "        'Replication factor': extract(r'Default replication factor:\\s+(\\d+)'),\n",
    "        'Status': 'HEALTHY' if 'HEALTHY' in text else 'CORRUPT' if 'CORRUPT' in text.upper() else 'UNKNOWN'\n",
    "    }\n",
    "\n",
    "# Cargar auditoría principal\n",
    "fsck_file = AUDIT_DIR / '2026-02-13' / 'fsck_data.txt'\n",
    "if fsck_file.exists():\n",
    "    fsck_text = fsck_file.read_text()\n",
    "    fsck_metrics = parse_fsck(fsck_text)\n",
    "    \n",
    "    print('=' * 50)\n",
    "    print('RESUMEN DE AUDITORÍA FSCK - /data')\n",
    "    print('=' * 50)\n",
    "    for k, v in fsck_metrics.items():\n",
    "        print(f'{k:25} : {v}')\n",
    "else:\n",
    "    print('No se encontró fsck_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e258bf62",
   "metadata": {},
   "source": [
    "## 3) Métricas de Replicación\n",
    "\n",
    "Comparación del impacto de diferentes factores de replicación (1, 2, 3) sobre el almacenamiento y tiempos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc991e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "IMPACTO DE REPLICACIÓN EN ALMACENAMIENTO\n",
      "==========================================================================================\n",
      " Factor  Lógico (MB)  Físico (MB)  Multiplicador  Tiempo (s)                             Tolerancia\n",
      "      1       376.33          376            1.0          41                         Sin tolerancia\n",
      "      2       376.33          752            2.0          11                         Tolera 1 fallo\n",
      "      3       376.33         1128            3.0          11 Tolera 1 fallo + re-replicación segura\n",
      "\n",
      "==================================================\n",
      "GRÁFICO: Espacio físico vs Factor de replicación\n",
      "==================================================\n",
      "Factor 1: █████████████ 376 MB\n",
      "Factor 2: ██████████████████████████ 752 MB\n",
      "Factor 3: ████████████████████████████████████████ 1128 MB\n"
     ]
    }
   ],
   "source": [
    "# Cargar métricas de replicación desde CSV\n",
    "rep_csv = METRICS_DIR / '2026-02-13' / 'replication_metrics.csv'\n",
    "\n",
    "if rep_csv.exists():\n",
    "    df_rep = pd.read_csv(rep_csv)\n",
    "    \n",
    "    # Añadir columnas calculadas\n",
    "    df_rep['Espacio lógico (MB)'] = (df_rep['logical_size_bytes'] / 1024 / 1024).round(2)\n",
    "    df_rep['Espacio físico (MB)'] = df_rep['physical_size_mb']\n",
    "    df_rep['Multiplicador'] = (df_rep['physical_size_bytes'] / df_rep['logical_size_bytes']).round(1)\n",
    "    df_rep['Tolerancia a fallos'] = df_rep['factor'].map({\n",
    "        1: 'Sin tolerancia',\n",
    "        2: 'Tolera 1 fallo',\n",
    "        3: 'Tolera 1 fallo + re-replicación segura'\n",
    "    })\n",
    "    \n",
    "    # Mostrar tabla formateada\n",
    "    print('=' * 90)\n",
    "    print('IMPACTO DE REPLICACIÓN EN ALMACENAMIENTO')\n",
    "    print('=' * 90)\n",
    "    display_df = df_rep[['factor', 'Espacio lógico (MB)', 'Espacio físico (MB)', \n",
    "                         'Multiplicador', 'time_setrep_sec', 'Tolerancia a fallos']].copy()\n",
    "    display_df.columns = ['Factor', 'Lógico (MB)', 'Físico (MB)', 'Multiplicador', 'Tiempo (s)', 'Tolerancia']\n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Mostrar gráfico de barras ASCII\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('GRÁFICO: Espacio físico vs Factor de replicación')\n",
    "    print('=' * 50)\n",
    "    max_val = df_rep['physical_size_mb'].max()\n",
    "    for _, row in df_rep.iterrows():\n",
    "        bar_len = int(40 * row['physical_size_mb'] / max_val)\n",
    "        bar = '█' * bar_len\n",
    "        print(f\"Factor {int(row['factor'])}: {bar} {int(row['physical_size_mb'])} MB\")\n",
    "else:\n",
    "    print('No se encontró replication_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "docker_stats_section",
   "metadata": {},
   "source": [
    "## 4) Uso de Recursos (Docker Stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "docker_stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "USO DE RECURSOS - DOCKER STATS\n",
      "===============================================================================================\n",
      "NAME                           CPU %     MEM USAGE / LIMIT     NET I/O           BLOCK I/O\n",
      "clustera-dnnm-3                0.32%     819.5MiB / 15.15GiB   1.65GB / 1.51GB   74.2MB / 1.65GB\n",
      "clustera-dnnm-1                0.40%     804.4MiB / 15.15GiB   1.65GB / 1.38GB   15.7MB / 1.65GB\n",
      "clustera-dnnm-2                0.46%     795.8MiB / 15.15GiB   1.65GB / 1.24GB   105MB / 1.65GB\n",
      "namenode                       0.15%     655.7MiB / 15.15GiB   830MB / 1.66GB    226MB / 4.42MB\n",
      "resourcemanager                0.42%     551.4MiB / 15.15GiB   1.94MB / 567kB    99.9MB / 1.68MB\n",
      "DB-ja-changes-andalucia-2024   0.00%     35.74MiB / 15.15GiB   1.63kB / 126B     55.2MB / 4.1kB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar docker stats\n",
    "stats_file = METRICS_DIR / '2026-02-13' / 'docker_stats.txt'\n",
    "\n",
    "if stats_file.exists():\n",
    "    stats_text = stats_file.read_text()\n",
    "    print('=' * 95)\n",
    "    print('USO DE RECURSOS - DOCKER STATS')\n",
    "    print('=' * 95)\n",
    "    print(stats_text)\n",
    "else:\n",
    "    print('No se encontró docker_stats.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inventory_section",
   "metadata": {},
   "source": [
    "## 5) Validación de Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "VALIDACIÓN DE BACKUP\n",
      "==================================================\n",
      "========== INFORME DE COMPARACIÓN ==========\n",
      "Fecha: 2026-02-13\n",
      "\n",
      "Ficheros en origen:  2\n",
      "Ficheros en backup:  2\n",
      "\n",
      "\n",
      "========== RESUMEN ==========\n",
      "Coincidentes: 2\n",
      "Missing:      0\n",
      "Mismatch:     0\n",
      "\n",
      "RESULTADO: BACKUP CONSISTENTE\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar informe de inventario\n",
    "inv_file = INVENTORY_DIR / '2026-02-13' / 'inventory_report_2026-02-13.txt'\n",
    "\n",
    "if inv_file.exists():\n",
    "    inv_text = inv_file.read_text()\n",
    "    print('=' * 50)\n",
    "    print('VALIDACIÓN DE BACKUP')\n",
    "    print('=' * 50)\n",
    "    print(inv_text)\n",
    "else:\n",
    "    print('No se encontró inventory_report.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident_section",
   "metadata": {},
   "source": [
    "## 6) Análisis del Incidente\n",
    "\n",
    "Comparación del estado ANTES y DESPUÉS de la caída simulada de un DataNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incident_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANÁLISIS DEL INCIDENTE - CAÍDA DE DATANODE\n",
      "======================================================================\n",
      "         Métrica   ANTES DESPUÉS\n",
      "       DataNodes       3       3\n",
      "          Status HEALTHY HEALTHY\n",
      "  Corrupt blocks       0       0\n",
      "  Missing blocks       0       0\n",
      "Under-replicated       0       0\n",
      "\n",
      "Nota: HDFS tarda ~10 minutos en detectar un DataNode como \"dead\".\n",
      "Durante ese periodo, los bloques aún aparecen como disponibles.\n"
     ]
    }
   ],
   "source": [
    "# Comparar auditorías pre y post incidente\n",
    "pre_file = AUDIT_DIR / '2026-02-13' / 'fsck_pre_incident.txt'\n",
    "post_file = AUDIT_DIR / '2026-02-13' / 'fsck_post_incident.txt'\n",
    "\n",
    "print('=' * 70)\n",
    "print('ANÁLISIS DEL INCIDENTE - CAÍDA DE DATANODE')\n",
    "print('=' * 70)\n",
    "\n",
    "if pre_file.exists() and post_file.exists():\n",
    "    pre_metrics = parse_fsck(pre_file.read_text())\n",
    "    post_metrics = parse_fsck(post_file.read_text())\n",
    "    \n",
    "    # Crear tabla comparativa\n",
    "    comparison = []\n",
    "    for key in ['DataNodes', 'Status', 'Corrupt blocks', 'Missing blocks', 'Under-replicated']:\n",
    "        comparison.append({\n",
    "            'Métrica': key,\n",
    "            'ANTES': pre_metrics.get(key, 'N/A'),\n",
    "            'DESPUÉS': post_metrics.get(key, 'N/A')\n",
    "        })\n",
    "    \n",
    "    df_incident = pd.DataFrame(comparison)\n",
    "    print(df_incident.to_string(index=False))\n",
    "    \n",
    "    print('\\nNota: HDFS tarda ~15 minutos en detectar un DataNode como \"dead\".')\n",
    "    print('Durante ese periodo, los bloques aún aparecen como disponibles.')\n",
    "else:\n",
    "    print('Archivos de incidente no encontrados.')\n",
    "    print('Ejecuta: bash scripts/70_incident_simulation.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## 7) Conclusiones y Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conclusions_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONCLUSIONES Y RECOMENDACIONES\n",
      "======================================================================\n",
      "\n",
      "FACTOR DE REPLICACIÓN RECOMENDADO: 3\n",
      "   - Con 3 DataNodes, replicación 3 garantiza que cada bloque esté en todos los nodos\n",
      "   - Coste: 3x espacio en disco (1128 MB físicos por 376 MB lógicos)\n",
      "   - Beneficio: Tolerancia a la caída de 1 nodo sin pérdida de datos\n",
      "   - La re-replicación automática protege ante fallos durante recuperación\n",
      "\n",
      "FRECUENCIA DE AUDITORÍA RECOMENDADA: Diaria\n",
      "   - `hdfs fsck` es una operación de solo lectura (no impacta rendimiento)\n",
      "   - Permite detectar bloques UNDER_REPLICATED o CORRUPT tempranamente\n",
      "   - En producción con mayor volumen: semanal + alertas automáticas del NameNode\n",
      "\n",
      "ESTRATEGIA DE BACKUP:\n",
      "   - Backup validado: 2 ficheros coincidentes, 0 missing, 0 mismatch\n",
      "   - Recomendación: Backup diario a /backup con validación por inventario\n",
      "   - Considerar backup a segundo clúster (DistCp) para DR real\n",
      "\n",
      "LECCIONES DEL INCIDENTE:\n",
      "   - HDFS detecta caída de DataNode en ~10 minutos (configurable)\n",
      "   - Con replicación 3, los datos permanecen accesibles durante el incidente\n",
      "   - La recuperación es automática al reiniciar el DataNode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('=' * 70)\n",
    "print('CONCLUSIONES Y RECOMENDACIONES')\n",
    "print('=' * 70)\n",
    "\n",
    "conclusions = '''\n",
    "FACTOR DE REPLICACIÓN RECOMENDADO: 3\n",
    "   - Con 3 DataNodes, replicación 3 garantiza que cada bloque esté en todos los nodos\n",
    "   - Coste: 3x espacio en disco (1128 MB físicos por 376 MB lógicos)\n",
    "   - Beneficio: Tolerancia a la caída de 1 nodo sin pérdida de datos\n",
    "   - La re-replicación automática protege ante fallos durante recuperación\n",
    "\n",
    "FRECUENCIA DE AUDITORÍA RECOMENDADA: Diaria\n",
    "   - `hdfs fsck` es una operación de solo lectura (no impacta rendimiento)\n",
    "   - Permite detectar bloques UNDER_REPLICATED o CORRUPT tempranamente\n",
    "   - En producción con mayor volumen: semanal + alertas automáticas del NameNode\n",
    "\n",
    "ESTRATEGIA DE BACKUP:\n",
    "   - Backup validado: 2 ficheros coincidentes, 0 missing, 0 mismatch\n",
    "   - Recomendación: Backup diario a /backup con validación por inventario\n",
    "   - Considerar backup a segundo clúster (DistCp) para DR real\n",
    "\n",
    "LECCIONES DEL INCIDENTE:\n",
    "   - HDFS detecta caída de DataNode en ~10 minutos (configurable)\n",
    "   - Con replicación 3, los datos permanecen accesibles durante el incidente\n",
    "   - La recuperación es automática al reiniciar el DataNode\n",
    "'''\n",
    "print(conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_table",
   "metadata": {},
   "source": [
    "## 8) Tabla Resumen de Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "final_table",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RESUMEN EJECUTIVO\n",
      "============================================================\n",
      "                        Métrica                                      Valor\n",
      "Tamaño total datos (logs + IoT)                ~779 MB (816,636,433 bytes)\n",
      "              Ficheros en /data 2 (logs_20260213.log + iot_20260213.jsonl)\n",
      "                Bloques totales                                         13\n",
      "          Factor de replicación                                          3\n",
      "              DataNodes activos                                          3\n",
      "              Bloques corruptos                                          0\n",
      "                Bloques missing                                          0\n",
      "       Bloques under-replicated                                          0\n",
      "          Estado del filesystem                                    HEALTHY\n",
      "                Backup validado                 Sí (2 ficheros, 0 errores)\n"
     ]
    }
   ],
   "source": [
    "# Tabla resumen final con datos reales\n",
    "summary_data = {\n",
    "    'Métrica': [\n",
    "        'Tamaño total datos (logs + IoT)',\n",
    "        'Ficheros en /data',\n",
    "        'Bloques totales',\n",
    "        'Factor de replicación',\n",
    "        'DataNodes activos',\n",
    "        'Bloques corruptos',\n",
    "        'Bloques missing',\n",
    "        'Bloques under-replicated',\n",
    "        'Estado del filesystem',\n",
    "        'Backup validado'\n",
    "    ],\n",
    "    'Valor': [\n",
    "        '~779 MB (816,636,433 bytes)',\n",
    "        '2 (logs_20260213.log + iot_20260213.jsonl)',\n",
    "        '13',\n",
    "        '3',\n",
    "        '3',\n",
    "        '0',\n",
    "        '0',\n",
    "        '0',\n",
    "        'HEALTHY',\n",
    "        'Sí (2 ficheros, 0 errores)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print('=' * 60)\n",
    "print('RESUMEN EJECUTIVO')\n",
    "print('=' * 60)\n",
    "print(df_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "export_csv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exportando tablas a CSV...\n",
      "Exportado: /media/notebooks/resumen_ejecutivo.csv\n",
      "Exportado: /media/notebooks/replicacion_comparativa.csv\n"
     ]
    }
   ],
   "source": [
    "# Exportar tablas a CSV\n",
    "print('\\nExportando tablas a CSV...')\n",
    "df_summary.to_csv('/media/notebooks/resumen_ejecutivo.csv', index=False)\n",
    "print('Exportado: /media/notebooks/resumen_ejecutivo.csv')\n",
    "\n",
    "if 'df_rep' in dir():\n",
    "    df_rep.to_csv('/media/notebooks/replicacion_comparativa.csv', index=False)\n",
    "    print('Exportado: /media/notebooks/replicacion_comparativa.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
